{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def7e687-8c22-4d76-a999-6fd6b0e5d96d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import timezone\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# --- Fun√ß√£o de conex√£o limpa ---\n",
    "def connect_supabase(base_url: str, api_key: str):\n",
    "    \"\"\"\n",
    "    Retorna uma sess√£o HTTP autenticada com o Supabase.\n",
    "    Retorna None se n√£o conseguir se conectar.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        session.headers.update({\n",
    "            \"apikey\": api_key,\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Prefer\": \"count=exact\"\n",
    "        })\n",
    "        resp = session.get(f\"{base_url}/rest/v1\", timeout=5)\n",
    "        if resp.status_code in (200, 404):  # 404 √© normal (endpoint sem tabela)\n",
    "            return session\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Fun√ß√£o principal ---\n",
    "def upsert_sales_btc_from_supabase(base_url: str, api_key: str, page_size: int = 1000, incremental: bool = True):\n",
    "    \"\"\"\n",
    "    L√™ dados de sales_btc no Supabase com pagina√ß√£o e insere apenas novos registros no Delta.\n",
    "    \"\"\"\n",
    "    CATALOG = \"lakehouse_imersao_jornada\"\n",
    "    SCHEMA  = \"postgres_public\"\n",
    "    FULL_TABLE = f\"{CATALOG}.{SCHEMA}.sales_btc\"\n",
    "\n",
    "    # 1Ô∏è‚É£ Conectar\n",
    "    session = connect_supabase(base_url, api_key)\n",
    "    if session is None:\n",
    "        print(\"‚ùå Falha na conex√£o com o Supabase.\")\n",
    "        return\n",
    "\n",
    "    print(\"‚úÖ Conex√£o estabelecida com o Supabase, carregando sales btc!!!!\")\n",
    "\n",
    "    # 2Ô∏è‚É£ Definir par√¢metros\n",
    "    params = {\"select\": \"*\", \"order\": \"importado_em.asc,transaction_id.asc\"}\n",
    "    if incremental and spark.catalog.tableExists(FULL_TABLE):\n",
    "        last_ts = spark.table(FULL_TABLE).select(F.max(\"importado_em\").alias(\"mx\")).collect()[0][\"mx\"]\n",
    "        if last_ts:\n",
    "            params[\"importado_em\"] = f\"gte.{last_ts.astimezone(timezone.utc).isoformat()}\"\n",
    "\n",
    "    # 3Ô∏è‚É£ Pagina√ß√£o\n",
    "    all_rows = []\n",
    "    start = 0\n",
    "    url = f\"{base_url}/rest/v1/sales_btc\"\n",
    "\n",
    "    while True:\n",
    "        end = start + page_size - 1\n",
    "        headers = {\"Range\": f\"{start}-{end}\"}\n",
    "        resp = session.get(url, params=params, headers=headers, timeout=60)\n",
    "\n",
    "        if resp.status_code not in (200, 206):\n",
    "            print(f\"‚ö†Ô∏è Erro {resp.status_code}: {resp.text[:200]}\")\n",
    "            break\n",
    "\n",
    "        batch = resp.json()\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        all_rows.extend(batch)\n",
    "        if len(batch) < page_size:\n",
    "            break\n",
    "        start += page_size\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"‚öôÔ∏è Nenhum dado novo retornado do Supabase.\")\n",
    "        return\n",
    "\n",
    "    # 4Ô∏è‚É£ Converter para Spark\n",
    "    pdf = pd.DataFrame(all_rows)\n",
    "    df = spark.createDataFrame(pdf) \\\n",
    "        .withColumn(\"data_hora\", F.to_timestamp(\"data_hora\")) \\\n",
    "        .withColumn(\"importado_em\", F.to_timestamp(\"importado_em\")) \\\n",
    "        .withColumn(\"quantidade\", F.col(\"quantidade\").cast(T.DecimalType(10, 4)))\n",
    "\n",
    "    # 5Ô∏è‚É£ Inserir apenas novos registros\n",
    "    if not spark.catalog.tableExists(FULL_TABLE):\n",
    "        df.write.mode(\"overwrite\").saveAsTable(FULL_TABLE)\n",
    "        print(f\"üÜï Tabela criada e {df.count()} linhas inseridas em {FULL_TABLE}.\")\n",
    "        return\n",
    "\n",
    "    df_existing = spark.table(FULL_TABLE).select(\"transaction_id\")\n",
    "    df_new_only = df.join(df_existing, on=\"transaction_id\", how=\"left_anti\")\n",
    "\n",
    "    new_count = df_new_only.count()\n",
    "    if new_count == 0:\n",
    "        print(\"‚úîÔ∏è Nenhum novo registro para inserir.\")\n",
    "    else:\n",
    "        df_new_only.write.mode(\"append\").saveAsTable(FULL_TABLE)\n",
    "        print(f\"‚úÖ {new_count} novos registros inseridos em {FULL_TABLE}.\")\n",
    "\n",
    "def upsert_sales_commodities_from_supabase(base_url: str, api_key: str, page_size: int = 1000, incremental: bool = True):\n",
    "    \"\"\"\n",
    "    L√™ dados de sales_commodities no Supabase (REST, paginado) e insere apenas novos registros no Delta.\n",
    "    Destino: lakehouse_imersao_jornada.postgres_public.sales_commodities\n",
    "    Requer a fun√ß√£o connect_supabase(base_url, api_key) definida previamente.\n",
    "    \"\"\"\n",
    "    CATALOG = \"lakehouse_imersao_jornada\"\n",
    "    SCHEMA  = \"postgres_public\"\n",
    "    FULL_TABLE = f\"{CATALOG}.{SCHEMA}.sales_commodities\"\n",
    "\n",
    "    # 1) Conectar (sess√£o autenticada)\n",
    "    session = connect_supabase(base_url, api_key)\n",
    "    if session is None:\n",
    "        print(\"‚ùå Falha na conex√£o com o Supabase.\")\n",
    "        return\n",
    "    print(\"‚úÖ Conex√£o estabelecida com o Supabase, carregando sales commodities!!!\")\n",
    "\n",
    "    # 2) Par√¢metros (incremental opcional)\n",
    "    params = {\"select\": \"*\", \"order\": \"importado_em.asc,transaction_id.asc\"}\n",
    "    if incremental and spark.catalog.tableExists(FULL_TABLE):\n",
    "        last_ts = spark.table(FULL_TABLE).select(F.max(\"importado_em\").alias(\"mx\")).collect()[0][\"mx\"]\n",
    "        if last_ts:\n",
    "            params[\"importado_em\"] = f\"gte.{last_ts.astimezone(timezone.utc).isoformat()}\"\n",
    "\n",
    "    # 3) Pagina√ß√£o via header Range\n",
    "    all_rows = []\n",
    "    start = 0\n",
    "    url = f\"{base_url}/rest/v1/sales_commodities\"\n",
    "\n",
    "    while True:\n",
    "        end = start + page_size - 1\n",
    "        headers = {\"Range\": f\"{start}-{end}\"}\n",
    "        resp = session.get(url, params=params, headers=headers, timeout=60)\n",
    "\n",
    "        if resp.status_code not in (200, 206):\n",
    "            print(f\"‚ö†Ô∏è Erro {resp.status_code}: {resp.text[:200]}\")\n",
    "            break\n",
    "\n",
    "        batch = resp.json()\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        all_rows.extend(batch)\n",
    "        if len(batch) < page_size:\n",
    "            break\n",
    "        start += page_size\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"‚öôÔ∏è Nenhum dado novo retornado do Supabase.\")\n",
    "        return\n",
    "\n",
    "    # 4) Pandas -> Spark + casts\n",
    "    pdf = pd.DataFrame(all_rows)\n",
    "    df = spark.createDataFrame(pdf) \\\n",
    "        .withColumn(\"data_hora\", F.to_timestamp(\"data_hora\")) \\\n",
    "        .withColumn(\"importado_em\", F.to_timestamp(\"importado_em\")) \\\n",
    "        .withColumn(\"quantidade\", F.col(\"quantidade\").cast(T.DecimalType(10, 4)))\n",
    "\n",
    "    # 5) Inserir apenas novos (anti-join pela PK)\n",
    "    if not spark.catalog.tableExists(FULL_TABLE):\n",
    "        df.write.mode(\"overwrite\").saveAsTable(FULL_TABLE)\n",
    "        print(f\"üÜï Tabela criada e {df.count()} linhas inseridas em {FULL_TABLE}.\")\n",
    "        return\n",
    "\n",
    "    df_existing = spark.table(FULL_TABLE).select(\"transaction_id\")\n",
    "    df_new_only = df.join(df_existing, on=\"transaction_id\", how=\"left_anti\")\n",
    "\n",
    "    new_count = df_new_only.count()\n",
    "    if new_count == 0:\n",
    "        print(\"‚úîÔ∏è Nenhum novo registro para inserir.\")\n",
    "    else:\n",
    "        df_new_only.write.mode(\"append\").saveAsTable(FULL_TABLE)\n",
    "        print(f\"‚úÖ {new_count} novos registros inseridos em {FULL_TABLE}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa63d9db-d9a6-4721-81bb-cecc56b99d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db_url = dbutils.secrets.get(scope=\"credencial-supabase\", key=\"url-supabase\")\n",
    "db_api = dbutils.secrets.get(scope=\"credencial-supabase\", key=\"api-supabase\")\n",
    "\n",
    "upsert_sales_btc_from_supabase(\n",
    "    base_url=db_url,\n",
    "    api_key=db_api,\n",
    "    page_size=1000,\n",
    "    incremental=True\n",
    ")\n",
    "\n",
    "upsert_sales_commodities_from_supabase(\n",
    "    base_url=db_url,\n",
    "    api_key=db_api,\n",
    "    page_size=1000,\n",
    "    incremental=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "get_sales_btc_commodities",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
